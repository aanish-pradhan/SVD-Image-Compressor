{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Command Line Utility For Lossy Image Compression via the Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** INTRODUCTION TO PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Singular Value Decomposition\n",
    "\n",
    "### What is the Singular Value Decomposition?\n",
    "\n",
    "The Singular Value Decomposition (SVD) is a ***powerful*** matrix decomposition technique in numerical linear algebra. It has a wide array of uses but is prolific in the field of machine learning (ML). The SVD can be used for \n",
    "\n",
    "- Solving $\\mathbf{Ax} = \\mathbf{b}$, especially in the context of regression\n",
    "- Principal Components Analysis (PCA) which allows for reducing high-dimensional data to lower-dimensional data by viewing key correlations withn it\n",
    "- Recommendation systems such as the Netflix algorithm or Google PageRank\n",
    "- Image recognition\n",
    "\n",
    "and much more. It can be thought of as a data-driven generalization of the Fourier Transform, another equally powerful technique. It is a data-driven generalization in the sense that is is unique to your data that you are working with at a given moment. The SVD is widely used because it depends on very simple linear algebra theory and can be applied to any dataset. Have a dataset? Apply the SVD!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does the SVD work?\n",
    "\n",
    "As mentioned earlier, the SVD is most practical when we have some kind of data. Suppose we have some arbitrary, $m \\times n$ data matrix $\\mathbf{X}$. For example, $\\mathbf{X}$ could be \"snapshots\" of some dynamical system through time (e.g. fluid flowing through a system) where each column vector of $\\mathbf{X}$ (i.e. $\\mathbf{x}_{i}$ for $i$ in the range 1 to $n$) is a flattened (matrix to vector) is a snapshot of the system at a point in time. Althernatively, it could be a dataset where each row is an observation and each column is a feature. In any case, the SVD guarantees that every such kind of matrix can be decomposed (a.k.a \"factorized\") into a product of three matrices:\n",
    "\n",
    "- $\\mathbf{U}$ - an orthonormal (unitary) $m \\times m$ matrix\n",
    "- $\\mathbf{\\Sigma}$ - a scalar (diagonal) $m \\times n$ matrix\n",
    "- $\\mathbf{V}^{T}$ - an orthonormal $n \\times n$ matrix\n",
    "\n",
    "Put together, $\\mathbf{X} = \\mathbf{U\\Sigma V}^{T}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Orthonormal & Unitary Matrices\n",
    "\n",
    "Any real-valued, $n \\times n$ matrix where the dot product between any two column vectors or row vectors is 0 and where the row and column vectors are of unit length are known as orthonormal (orthogonal) matrices. Specifically, an orthonormal matrix satisfies the following properties\n",
    "\n",
    "$$\n",
    "    \\mathbf{A}^{T}\\mathbf{A} = \\mathbf{A}\\mathbf{A}^{T} = \\mathbf{I}_{n} \\\\\n",
    "    \\mathbf{A}^{T} = \\mathbf{A}^{-1}\n",
    "$$\n",
    "\n",
    "A complex-valued matrix that satisfies the complex-value analog of this property is a unitary matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The $\\mathbf{U}$ matrix\n",
    "\n",
    "The $\\mathbf{U}$ matrix is called the \"left singular vector matrix\". It is an $m \\times m$ matrix composed of $m$ column vectors known as \"singular vectors\" as follows\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        | & | & \\, & | \\\\\n",
    "        \\mathbf{u}_{1} & \\mathbf{u}_{2} & \\cdots & \\mathbf{u}_{m} \\\\\n",
    "        | & | & \\, & |\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "These vectors are arranged heirarcically; $\\mathbf{u}_{1}$ is more important to $\\mathbf{X}$ than $\\mathbf{u}_{2}$, $\\mathbf{u}_{2}$ is more important to $\\mathbf{X}$ than $\\mathbf{u}_{3}$, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The $\\mathbf{\\Sigma}$ matrix\n",
    "\n",
    "The $\\mathbf{\\Sigma}$ matrix is called the \"sigma\" matrix. It is an $m \\times n$ scalar (a.k.a. diagonal) matrix composed of $n$, postiive, scalars called \"singular values\" that form the primary diagonal as follows\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        \\sigma_{1} & 0 & \\cdots & 0 \\\\\n",
    "        0 & \\sigma_{2} & \\cdots & 0 \\\\\n",
    "        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "        0 & 0 & \\cdots & \\sigma_{n} \\\\\n",
    "        0 & 0 & \\cdots & 0 \\\\\n",
    "        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "        0 & 0 & \\cdots & 0\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Entries below the first $n$ rows are padded with 0s. Similar to $\\mathbf{U}$, the singular values in $\\mathbf{\\Sigma}$ are arranged heirarchically. Specifically, the singular values are arranged with decreasing value (i.e. $\\sigma_{1} \\geq \\sigma_{2} \\geq \\cdots \\geq \\sigma_{n} \\geq 0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The $\\mathbf{V}^{T}$ matrix\n",
    "\n",
    "The $\\mathbf{V}^{T}$ matrix is called the \"right singular vector matrix\". It is an $n \\times n$ matrix composed of $n$ column vectors transposed into row vectors as follows\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        \\text{---}& \\mathbf{v}_{1}^{T} & \\text{---} \\\\\n",
    "        \\text{---} & \\mathbf{v}_{2}^{T} & \\text{---} \\\\\n",
    "        \\text{---} & \\vdots & \\text{---} \\\\\n",
    "        \\text{---} & \\mathbf{v}_{n}^{T} & \\text{---}\n",
    "    \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallels to the Eigendecomposition.\n",
    "\n",
    "If we apply the previously mentioned properties of orthonormal matrices to our formula for the SVD, we obtain\n",
    "\n",
    "$$\n",
    "    \\mathbf{X} = \\mathbf{U\\Sigma V}^{-1}\n",
    "$$\n",
    "\n",
    "In this form, the SVD closely resembles the eigendecomposition of a matrix. Recall, that the eigendecomposition (a.k.a. diagonalization) is a technique that allows one to decompose a square $n \\times n$ matrix $\\mathbf{A}$ into a product of three other matrices:\n",
    "\n",
    "- $\\mathbf{P}$ - a square, $n \\times n$, matrix \n",
    "- $\\mathbf{D}$ - a square, $n \\times n$, scalar matrix\n",
    "- $\\mathbf{P}^{-1}$ - a square, $n \\times n$, matrix\n",
    "\n",
    "Put together, $\\mathbf{A} = \\mathbf{PDP}^{-1}$.\n",
    "\n",
    "$\\mathbf{P}$ contains the eigenvectors of $\\mathbf{A}$, $\\mathbf{D}$ contains the corresponding eigenvalues to each eigenvector and $\\mathbf{P}^{-1}$ is the inverse of $\\mathbf{P}$. Note, this decomposition is only possible if the algebraic and geometric multiplicities of each eigenvalue of $\\mathbf{A}$.\n",
    "\n",
    "Recall that the eigendecomposition is only possible for square matrices while the SVD is a generalization for any $m \\times n$ matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expansion of the SVD as a Summation\n",
    "\n",
    "If we begin to expand the matrix and vector multiplications of the SVD, we start to obtain the following\n",
    "\n",
    "$$\n",
    "    \\sigma_{1}\\mathbf{u}_{1}\\mathbf{v}_{1}^{T} + \\sigma_{2}\\mathbf{u}_{2}\\mathbf{v}_{2}^{T} + \\cdots \n",
    "$$\n",
    "\n",
    "Any $\\sigma_{i}\\mathbf{u}_{i}$ returns a scaled left singular vector with the same dimensions as $\\mathbf{u}_{i}$. A column vector times a transposed column vector (i.e. a row vector) is an outer product operation. That yields a rank 1 $m \\times n$ matrix. If we progressively add the rank 1 matrices returned by the product of any $\\sigma_{i}\\mathbf{u}_{i}\\mathbf{v}^{T}$, we can build the original data matrix $\\mathbf{X}$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "However, we notice something. Our data matrix $\\mathbf{X}$ has $n$ column vectors. Meaning, it can have at most $n$ linearly independent column vectors that span $\\mathbb{R}^{m}$. However, $\\mathbf{U}$ contains $m$ left singular vectors and $\\mathbf{\\Sigma}$ only contains $n$ singular values. As a result, we only require the first $n$ left singular vectors of $\\mathbf{U}$ to reconstruct $\\mathbf{X}$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "0ac3ca1ccb93166e40abfdb2edb9b96aa9a9e089130d5b3075f45d7ec5be5b97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
